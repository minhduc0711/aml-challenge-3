{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09139506-3282-4511-b0ef-9cd055714f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891d4c8a-dfc9-46e1-9f9f-beae33df4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "embeddings_index = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326c2ca8-fae7-44e8-b28f-683e876fa463",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_origin = pd.read_csv(\"../data/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75c72e8-081e-4dda-96e4-68752bda34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Count word in the dataset\n",
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {} # Word in vocab\n",
    "    oov = {}\n",
    "    k = 0 # Total number of word in the embedding index\n",
    "    i = 0 # Total number of word not in the embedding index\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word] # Find word in pre-trained embedding index\n",
    "            k += vocab[word] \n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "def remove_url(x):\n",
    "    x = re.sub(r'http\\S+', '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c09e10-fd1c-4f86-86bd-7c61c59abc20",
   "metadata": {},
   "source": [
    "# Remove url and lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f741789-5092-4492-8e17-393bb0a32f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 260229.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 183276.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 174176.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 151604.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create function for lowercasing since glove is containing all lowercase token\n",
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: x.lower())\n",
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: remove_url(x))\n",
    "sentences = pd_train_origin[\"text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a64844-603e-429d-b19b-3914b8dc7997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 41095/41095 [00:00<00:00, 196261.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 36.87% of vocab\n",
      "Found embeddings for  81.87% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515d6f61-6e87-4eb9-afe7-6fac0532090b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i`m', 1959),\n",
       " ('it`s', 1068),\n",
       " ('don`t', 764),\n",
       " ('****', 719),\n",
       " ('can`t', 675),\n",
       " ('i`ll', 387),\n",
       " ('that`s', 353),\n",
       " ('mother`s', 328),\n",
       " ('didn`t', 317),\n",
       " ('i`ve', 305)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b85b7c-d1db-41f0-b830-e7e32acd06d3",
   "metadata": {},
   "source": [
    "# Handle emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d7700b-6387-4fac-bc84-29e43406d1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 150061.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{';)': 120,\n",
       " ':]': 18,\n",
       " ':/': 33,\n",
       " ':3': 53,\n",
       " 'd:': 37,\n",
       " ':@': 12,\n",
       " '=/': 8,\n",
       " '*)': 5,\n",
       " ':|': 16,\n",
       " '=]': 7,\n",
       " ':>': 3,\n",
       " ':o': 30,\n",
       " '0:3': 8,\n",
       " ':o)': 2,\n",
       " ':$': 1,\n",
       " '=p': 9,\n",
       " ':[': 6,\n",
       " ';;': 5,\n",
       " '8-)': 2,\n",
       " ':*': 5,\n",
       " ':-0': 1,\n",
       " \":'(\": 1,\n",
       " ':b': 2,\n",
       " '=3': 1,\n",
       " ':{': 1,\n",
       " \":')\": 1,\n",
       " '%)': 3,\n",
       " ':x': 1,\n",
       " ':-*': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling with emoji\n",
    "import emot\n",
    "\n",
    "emot_obj = emot.core.emot() \n",
    "\n",
    "# get and count emoji from corpus\n",
    "def build_vocab_emoji(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        # get emoticons and emoji\n",
    "        emojis = emot_obj.emoticons(sentence)['value']\n",
    "        if len(emojis) != 0:\n",
    "            for emoji in emojis:\n",
    "                try:\n",
    "                    vocab[emoji] += 1\n",
    "                except KeyError:\n",
    "                    vocab[emoji] = 1\n",
    "    return vocab\n",
    "                                          \n",
    "vocab_emoji = build_vocab_emoji(pd_train_origin[\"text\"])\n",
    "vocab_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0ccb92a-2136-47ca-b2fa-40d955d1dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove *), 0:3, :o) - not an emoji following the context of sentence\n",
    "\n",
    "# vocab_emoji.pop('*)')\n",
    "# vocab_emoji.pop('0:3')\n",
    "# vocab_emoji.pop(':3')\n",
    "# vocab_emoji.pop(':o)')\n",
    "# vocab_emoji.pop(';;')\n",
    "# vocab_emoji.pop('%)')\n",
    "# vocab_emoji.pop('=3')\n",
    "\n",
    "vocab_emoji = {\n",
    "    ';)': 'smirk',\n",
    "    ':]': 'smiley',\n",
    "    ':/': 'skeptical',\n",
    "    'd:': 'cheeky',\n",
    "    ':@': 'sad',\n",
    "    '=/': 'annoyed',\n",
    "    ':|': 'neutral',\n",
    "    '=]': 'happy',\n",
    "    ':>': 'happy',\n",
    "    ':o': 'surprise',\n",
    "    ':$': 'blushing',\n",
    "    '=p': 'cheeky',\n",
    "    ':[': 'sad',\n",
    "    '8-)': 'happy',\n",
    "    ':*': 'kiss',\n",
    "    ':-0': 'shock',\n",
    "    \":'(\": 'crying',\n",
    "    \":b\": 'cheeky',\n",
    "    \":{\": 'sad',\n",
    "    \":')\": 'sad',\n",
    "    ':x': 'mute',\n",
    "    ':-*': 'kiss',\n",
    "}\n",
    "        \n",
    "def fix_emoji(x):\n",
    "    emojis = emot_obj.emoticons(x)\n",
    "    for value in emojis['value']:\n",
    "        if value in vocab_emoji:\n",
    "            x = x.replace(value, vocab_emoji[value])\n",
    "        else:\n",
    "            continue\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a840dd38-8254-45d1-b56f-65e17fecbc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 105377.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 148127.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 41076/41076 [00:00<00:00, 213933.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 36.88% of vocab\n",
      "Found embeddings for  81.90% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: fix_emoji(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)\n",
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f15f901-25a8-4d65-a429-0f132964c6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 141710.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{':3': 53, '*)': 5, '0:3': 8, ':o)': 2, ';;': 5, '=3': 1, '%)': 3}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_emoji = build_vocab_emoji(pd_train_origin[\"text\"])\n",
    "vocab_emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988b0fc-4c76-4330-a280-1afa3d4bc930",
   "metadata": {},
   "source": [
    "# Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7af0923-b950-4fd0-ad22-69305379e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the punnctuation and other stuff\n",
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21b38a63-2be7-4bc8-89b5-c46d5f11b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 65494.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 156191.32it/s]\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb6e98d-b68d-4b6a-b7f9-30fac2bf1cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25608/25608 [00:00<00:00, 184326.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 68.20% of vocab\n",
      "Found embeddings for  96.73% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad97ca01-5bbc-46b8-afdd-af0a39f1cca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hahaha', 94),\n",
       " ('lmao', 65),\n",
       " ('bday', 48),\n",
       " ('youve', 45),\n",
       " ('itll', 36),\n",
       " ('idk', 36),\n",
       " ('hahah', 28),\n",
       " ('followfriday', 26),\n",
       " ('iï¿½m', 26),\n",
       " ('thanx', 25)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special characters and internet slang are not vectorize in Glove\n",
    "# Some words can be fix? hahahah is one example which we can reduce it down to ha or haha\n",
    "# And fix some common misspellings \n",
    "# Needs to find a way to solve abbreviated words\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022719a-ce73-4bda-961a-54699df478a8",
   "metadata": {},
   "source": [
    "# Fix laughing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01d9afbd-58b5-4237-a8a1-8ce2fd1a9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_duplicate_words(x):\n",
    "    # Change hahahahaha or lolololo to haha\n",
    "    x = re.sub(r'\\b(?:a*(?:ha)+h?|(?:l+o+)+l+)\\b', 'haha', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c0fdea1-e800-40ec-a094-403e060945e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 60710.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 155814.07it/s]\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: fix_duplicate_words(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f76daefd-2b3b-4460-a370-9574755c63f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25577/25577 [00:00<00:00, 168623.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 68.26% of vocab\n",
      "Found embeddings for  96.79% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lmao', 65),\n",
       " ('bday', 48),\n",
       " ('youve', 45),\n",
       " ('itll', 36),\n",
       " ('idk', 36),\n",
       " ('followfriday', 26),\n",
       " ('iï¿½m', 26),\n",
       " ('thanx', 25),\n",
       " ('ï¿½', 20),\n",
       " ('shouldnt', 19)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79be63-2bb5-4dc9-80b8-e702b8455aef",
   "metadata": {},
   "source": [
    "# Fix misspelling word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11ef4cd1-6b0d-4126-ae26-e91dfd0afbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelling = {\n",
    "    'bday': 'birthday',\n",
    "    'itll': 'it will',\n",
    "    'youve': 'you have',\n",
    "    'idk': 'i do not know',\n",
    "    'followfriday': 'follow friday',\n",
    "    'shouldnt': 'should not',\n",
    "    'tonights': 'tonight',\n",
    "    'sux': 'suck',\n",
    "    'mommys': 'mommy',\n",
    "    'werent': 'were not',\n",
    "    'everyones': 'everyone',\n",
    "    'theyve': 'they have',\n",
    "    'lmao': 'haha',\n",
    "    'LMAO': 'haha',\n",
    "    'awsome': 'awesome',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a27e01f-0e34-45cd-8884-953080e391c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix soooooo to so and Lmao\n",
    "def fix_word(x):\n",
    "    # fix soooooo to so    \n",
    "    x = re.sub(r'\\b(?:s+o+)+\\b', 'so', x)\n",
    "    # fix lmao and LMAO to haha\n",
    "    for word in x.split():\n",
    "        if word in misspelling.keys():\n",
    "            x = x.replace(word, misspelling[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a0230c6-8136-4747-b6c6-e7d1ff194890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 52490.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 152609.45it/s]\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: fix_word(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baa37807-90b8-4299-85fd-f89177b9d5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lmao', 65),\n",
       " ('bday', 48),\n",
       " ('youve', 45),\n",
       " ('itll', 36),\n",
       " ('idk', 36),\n",
       " ('followfriday', 26),\n",
       " ('iï¿½m', 26),\n",
       " ('thanx', 25),\n",
       " ('ï¿½', 20),\n",
       " ('shouldnt', 19)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f1740a-f326-4a2b-8d14-0d73e79ec4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
