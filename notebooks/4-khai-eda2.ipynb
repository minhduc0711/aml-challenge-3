{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09139506-3282-4511-b0ef-9cd055714f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "891d4c8a-dfc9-46e1-9f9f-beae33df4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "embeddings_index = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "326c2ca8-fae7-44e8-b28f-683e876fa463",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_origin = pd.read_csv(\"../data/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d75c72e8-081e-4dda-96e4-68752bda34e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Count word in the dataset\n",
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {} # Word in vocab\n",
    "    oov = {}\n",
    "    k = 0 # Total number of word in the embedding index\n",
    "    i = 0 # Total number of word not in the embedding index\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word] # Find word in pre-trained embedding index\n",
    "            k += vocab[word] \n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "def remove_url(x):\n",
    "    x = re.sub(r'http\\S+', '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c09e10-fd1c-4f86-86bd-7c61c59abc20",
   "metadata": {},
   "source": [
    "# Remove url and lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f741789-5092-4492-8e17-393bb0a32f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 360030.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 275697.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 213360.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 149744.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create function for lowercasing since glove is containing all lowercase token\n",
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: x.lower())\n",
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: remove_url(x))\n",
    "sentences = pd_train_origin[\"text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c5a64844-603e-429d-b19b-3914b8dc7997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 41095/41095 [00:00<00:00, 266370.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 36.87% of vocab\n",
      "Found embeddings for  81.87% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "515d6f61-6e87-4eb9-afe7-6fac0532090b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i`m', 1959),\n",
       " ('it`s', 1068),\n",
       " ('don`t', 764),\n",
       " ('****', 719),\n",
       " ('can`t', 675),\n",
       " ('i`ll', 387),\n",
       " ('that`s', 353),\n",
       " ('mother`s', 328),\n",
       " ('didn`t', 317),\n",
       " ('i`ve', 305)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea5370-cdeb-4b3e-9e16-c8ad81cccaa4",
   "metadata": {},
   "source": [
    "# Remove time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d6041db3-354e-4337-9372-ffa196112d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_time(x):\n",
    "    x = re.sub('\\d+:\\d+', '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5b06a261-4bda-4dc2-acf3-1d22060344c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 149057.41it/s]\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: remove_time(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b85b7c-d1db-41f0-b830-e7e32acd06d3",
   "metadata": {},
   "source": [
    "# Handle emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d9d7700b-6387-4fac-bc84-29e43406d1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 172675.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{';)': 120,\n",
       " ':]': 18,\n",
       " ':/': 33,\n",
       " 'd:': 37,\n",
       " ':@': 12,\n",
       " '=/': 8,\n",
       " '*)': 5,\n",
       " ':|': 16,\n",
       " '=]': 7,\n",
       " ':>': 3,\n",
       " ':o': 30,\n",
       " ':o)': 2,\n",
       " ':$': 1,\n",
       " '=p': 9,\n",
       " ':[': 6,\n",
       " ';;': 5,\n",
       " '8-)': 2,\n",
       " ':*': 5,\n",
       " ':3': 4,\n",
       " ':-0': 1,\n",
       " \":'(\": 1,\n",
       " ':b': 2,\n",
       " '=3': 1,\n",
       " ':{': 1,\n",
       " \":')\": 1,\n",
       " '%)': 3,\n",
       " ':x': 1,\n",
       " ':-*': 1}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling with emoji\n",
    "import emot\n",
    "\n",
    "emot_obj = emot.core.emot() \n",
    "\n",
    "# get and count emoji from corpus\n",
    "def build_vocab_emoji(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        # get emoticons and emoji\n",
    "        emojis = emot_obj.emoticons(sentence)['value']\n",
    "        if len(emojis) != 0:\n",
    "            for emoji in emojis:\n",
    "                try:\n",
    "                    vocab[emoji] += 1\n",
    "                except KeyError:\n",
    "                    vocab[emoji] = 1\n",
    "    return vocab\n",
    "                                          \n",
    "vocab_emoji = build_vocab_emoji(pd_train_origin[\"text\"])\n",
    "vocab_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad90c940-03ce-4df8-9491-b9f2d54e708a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': [], 'location': [], 'mean': [], 'flag': False}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emot_obj.emoticons('<3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f0ccb92a-2136-47ca-b2fa-40d955d1dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove *), 0:3, :o) - not an emoji following the context of sentence\n",
    "\n",
    "# vocab_emoji.pop('*)')\n",
    "# vocab_emoji.pop('0:3')\n",
    "# vocab_emoji.pop(':o)')\n",
    "# vocab_emoji.pop(';;')\n",
    "# vocab_emoji.pop('%)')\n",
    "# vocab_emoji.pop('=3')\n",
    "\n",
    "vocab_emoji = {\n",
    "    ';)': 'smirk',\n",
    "    ':]': 'smiley',\n",
    "    ':/': 'skeptical',\n",
    "    'd:': 'cheeky',\n",
    "    ':@': 'sad',\n",
    "    '=/': 'annoyed',\n",
    "    ':|': 'neutral',\n",
    "    '=]': 'happy',\n",
    "    ':>': 'happy',\n",
    "    ':o': 'surprise',\n",
    "    ':$': 'blushing',\n",
    "    '=p': 'cheeky',\n",
    "    ':[': 'sad',\n",
    "    '8-)': 'happy',\n",
    "    ':*': 'kiss',\n",
    "    ':-0': 'shock',\n",
    "    \":'(\": 'crying',\n",
    "    \":b\": 'cheeky',\n",
    "    \":{\": 'sad',\n",
    "    \":')\": 'sad',\n",
    "    ':x': 'mute',\n",
    "    ':-*': 'kiss',\n",
    "    ':3': 'happy'\n",
    "}\n",
    "        \n",
    "def fix_emoji(x):\n",
    "    emojis = emot_obj.emoticons(x)\n",
    "    for value in emojis['value']:\n",
    "        if value in vocab_emoji:\n",
    "            x = x.replace(value, vocab_emoji[value])\n",
    "        else:\n",
    "            continue\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a840dd38-8254-45d1-b56f-65e17fecbc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 130354.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 170652.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 40963/40963 [00:00<00:00, 280314.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 36.84% of vocab\n",
      "Found embeddings for  81.91% of all text\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: fix_emoji(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)\n",
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8f15f901-25a8-4d65-a429-0f132964c6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 166332.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'*)': 5, ':o)': 2, ';;': 5, '=3': 1, '%)': 3}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_emoji = build_vocab_emoji(pd_train_origin[\"text\"])\n",
    "vocab_emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb488fcf-08ce-4f57-a453-e9463dc9a4c9",
   "metadata": {},
   "source": [
    "# Fix heart emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b211d68e-20f0-47fd-b0bf-3b4d0cf92b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_heart(x):\n",
    "    x = re.sub('<3+', 'love', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "96b87547-83e0-428b-aacf-1ba91b4d81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 258362.37it/s]\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin['text'] = pd_train_origin['text'].progress_apply(lambda x: fix_heart(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff5345-2be4-46a5-a270-7d87e9e33715",
   "metadata": {},
   "source": [
    "# Resolve contraction and slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "46dce828-3d82-4cc8-9498-a95a39409d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 168958.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 56694.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 163335.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "from textacy.preprocessing.normalize import quotation_marks\n",
    "\n",
    "pd_train_origin['text'] = pd_train_origin['text'].progress_apply(lambda x: quotation_marks(x))\n",
    "pd_train_origin['text'] = pd_train_origin['text'].progress_apply(lambda x: contractions.fix(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7c4a4857-97ef-47ad-9d57-2e4d98135302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 40585/40585 [00:00<00:00, 274404.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 37.02% of vocab\n",
      "Found embeddings for  85.14% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bae8060e-9d53-4155-ac47-77214bda8eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('****', 719),\n",
       " (\"mother's\", 328),\n",
       " ('it.', 240),\n",
       " ('day!', 173),\n",
       " ('now.', 165),\n",
       " ('today.', 154),\n",
       " ('you!', 153),\n",
       " ('you.', 149),\n",
       " ('it!', 145),\n",
       " ('day.', 125),\n",
       " ('..', 117),\n",
       " ('yeah,', 115),\n",
       " ('too.', 112),\n",
       " ('though.', 108),\n",
       " ('well,', 103),\n",
       " ('you?', 98),\n",
       " ('lol.', 96),\n",
       " ('it,', 95),\n",
       " ('me!', 94),\n",
       " ('tomorrow.', 90)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988b0fc-4c76-4330-a280-1afa3d4bc930",
   "metadata": {},
   "source": [
    "# Remove symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b7af0923-b950-4fd0-ad22-69305379e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the punnctuation and other stuff\n",
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "21b38a63-2be7-4bc8-89b5-c46d5f11b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 99596.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 167276.26it/s]\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0bb6e98d-b68d-4b6a-b7f9-30fac2bf1cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25168/25168 [00:00<00:00, 226080.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 68.78% of vocab\n",
      "Found embeddings for  97.06% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ad97ca01-5bbc-46b8-afdd-af0a39f1cca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hahaha', 94),\n",
       " ('lmao', 65),\n",
       " ('hahah', 28),\n",
       " ('followfriday', 26),\n",
       " ('iï¿½m', 26),\n",
       " ('thanx', 25),\n",
       " ('ï¿½', 20),\n",
       " ('hahahaha', 20),\n",
       " ('tweeps', 15),\n",
       " ('2moro', 14)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special characters and internet slang are not vectorize in Glove\n",
    "# Some words can be fix? hahahah is one example which we can reduce it down to ha or haha\n",
    "# And fix some common misspellings \n",
    "# Needs to find a way to solve abbreviated words\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "152f21a7-16e7-4a74-a8af-373c6f43d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 144233.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_emoji = build_vocab_emoji(pd_train_origin[\"text\"])\n",
    "vocab_emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022719a-ce73-4bda-961a-54699df478a8",
   "metadata": {},
   "source": [
    "# Fix laughing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f56ea6eb-f334-41a9-bce4-ecb953c7bf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hahahahaha i rember when i riped that william picture out of one of claires mags i beated zoe to it'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train_origin[\"text\"][44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "01d9afbd-58b5-4237-a8a1-8ce2fd1a9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_duplicate_words(x):\n",
    "    # Change hahahahaha or lolololo to haha\n",
    "    x = re.sub(r'\\b(?:a*(?:ha)+h?|(?:l+o+)+l+)\\b', 'haha', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2c0fdea1-e800-40ec-a094-403e060945e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 84208.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 175681.97it/s]\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: fix_duplicate_words(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f76daefd-2b3b-4460-a370-9574755c63f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25137/25137 [00:00<00:00, 262327.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 68.84% of vocab\n",
      "Found embeddings for  97.12% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lmao', 65),\n",
       " ('followfriday', 26),\n",
       " ('iï¿½m', 26),\n",
       " ('thanx', 25),\n",
       " ('ï¿½', 20),\n",
       " ('tweeps', 15),\n",
       " ('2moro', 14),\n",
       " ('awsome', 14),\n",
       " ('soooooo', 14),\n",
       " ('tooo', 13)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79be63-2bb5-4dc9-80b8-e702b8455aef",
   "metadata": {},
   "source": [
    "# Fix misspelling word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "11ef4cd1-6b0d-4126-ae26-e91dfd0afbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelling = {\n",
    "    'bday': 'birthday',\n",
    "    'itll': 'it will',\n",
    "    'youve': 'you have',\n",
    "    'idk': 'i do not know',\n",
    "    'followfriday': 'follow friday',\n",
    "    'shouldnt': 'should not',\n",
    "    'tonights': 'tonight',\n",
    "    'sux': 'suck',\n",
    "    'mommys': 'mommy',\n",
    "    'werent': 'were not',\n",
    "    'everyones': 'everyone',\n",
    "    'theyve': 'they have',\n",
    "    'lmao': 'haha',\n",
    "    'LMAO': 'haha',\n",
    "    'awsome': 'awesome',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9a27e01f-0e34-45cd-8884-953080e391c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix soooooo to so and Lmao\n",
    "def fix_word(x):\n",
    "    # fix soooooo to so    \n",
    "    x = re.sub(r'\\b(?:s+o+)+\\b', 'so', x)\n",
    "    # fix lmao and LMAO to haha\n",
    "    for word in x.split():\n",
    "        if word in misspelling.keys():\n",
    "            x = x.replace(word, misspelling[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4a0230c6-8136-4747-b6c6-e7d1ff194890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 67733.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 24732/24732 [00:00<00:00, 179766.48it/s]\n"
     ]
    }
   ],
   "source": [
    "pd_train_origin[\"text\"] = pd_train_origin[\"text\"].progress_apply(lambda x: fix_word(x))\n",
    "sentences = pd_train_origin[\"text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "baa37807-90b8-4299-85fd-f89177b9d5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lmao', 65),\n",
       " ('followfriday', 26),\n",
       " ('iï¿½m', 26),\n",
       " ('thanx', 25),\n",
       " ('ï¿½', 20),\n",
       " ('tweeps', 15),\n",
       " ('2moro', 14),\n",
       " ('awsome', 14),\n",
       " ('soooooo', 14),\n",
       " ('tooo', 13)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f5f1740a-f326-4a2b-8d14-0d73e79ec4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lmao'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(\"lmao\", slang=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf8e82-e750-49fb-92f3-f56f5b13dc55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
