{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cf927b5-3206-4cc2-b842-9e1a9d4713af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fa18c0-24c4-4993-b1b4-f1ec2abc22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254f7712-eec3-4e83-a874-c6433b551b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73a0b5f-aa3b-4e83-825b-81a3d2f07941",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_count_vectorizer\n\u001b[1;32m      6\u001b[0m tokenize_fn \u001b[38;5;241m=\u001b[39m word_tokenize\n\u001b[0;32m----> 8\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m [tokenize_fn(preprocess(text)) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      9\u001b[0m embedding_fn \u001b[38;5;241m=\u001b[39m get_count_vectorizer(train_tokens)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_count_vectorizer\n\u001b[1;32m      6\u001b[0m tokenize_fn \u001b[38;5;241m=\u001b[39m word_tokenize\n\u001b[0;32m----> 8\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      9\u001b[0m embedding_fn \u001b[38;5;241m=\u001b[39m get_count_vectorizer(train_tokens)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nltk/tokenize/destructive.py:182\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    179\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 182\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    184\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from src.features.preprocess import preprocess\n",
    "from src.features.vectorizer import get_count_vectorizer\n",
    "\n",
    "tokenize_fn = word_tokenize\n",
    "\n",
    "train_tokens = [tokenize_fn(preprocess(text)) for text in train_df[\"text\"]]\n",
    "embedding_fn = get_count_vectorizer(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "042d3189-4718-42bd-aa86-20d55ff14e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import TextDataset\n",
    "\n",
    "ds = TextDataset(\"../data/raw/train.csv\",\n",
    "                 preprocess_fn=preprocess,\n",
    "                 tokenize_fn=tokenize_fn,\n",
    "                 vectorizer_fn=embedding_fn, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6afe6a2b-6ded-42ed-b159-82d027d4d2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22259, 2473)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ratio = 0.1\n",
    "num_val = int(val_ratio * len(ds))\n",
    "num_train = len(ds) - num_val\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [num_train, num_val])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4902e8f-a8d9-4fd7-abb4-3f3b41f94865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfe09a6fe074c1b95b70c1759d10ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tqdm.auto import tqdm\n",
    "clf = MultinomialNB()\n",
    "train_loader = DataLoader(train_ds, batch_size=1024, shuffle=False)\n",
    "for batch in tqdm(train_loader):\n",
    "    X_batch, y_batch = batch\n",
    "    clf.partial_fit(X_batch.to_dense().numpy(), y_batch.numpy(), classes=[0, 1, 2])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81150cad-20ba-4793-89ee-4c66522a8666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98c0ffb036b4e86aa9a14e81114d9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6332389809947432"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader = DataLoader(val_ds, batch_size=512, shuffle=False)\n",
    "corrects = []\n",
    "for batch in tqdm(val_loader):\n",
    "    X_batch, y_batch = batch\n",
    "    y_batch = y_batch.numpy()\n",
    "    y_pred = clf.predict(X_batch.to_dense().numpy())\n",
    "    corrects.append(y_pred == y_batch)\n",
    "corrects= np.concatenate(corrects)\n",
    "corrects.sum() / len(corrects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5d5b3561-320b-4de7-9a5c-a91f19e8d1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac55aca34474b528202334bf6f45bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrain on full dataset\n",
    "clf = MultinomialNB()\n",
    "train_loader = DataLoader(ds, batch_size=512, shuffle=False)\n",
    "for batch in tqdm(train_loader):\n",
    "    X_batch, y_batch = batch\n",
    "    clf.partial_fit(X_batch.to_dense().numpy(), y_batch.numpy(), classes=[0, 1, 2])\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e884f575-5441-4406-9a3d-0bc7af6be8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f7904146fc45ed97e251c58091b76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on test\n",
    "test_ds = TextDataset(\"../data/raw/test.csv\",\n",
    "                 preprocess_fn=preprocess,\n",
    "                 tokenize_fn=tokenize_fn,\n",
    "                 embedding_fn=embedding_fn, sparse=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=512, shuffle=False)\n",
    "\n",
    "preds = []\n",
    "for batch in tqdm(test_loader):\n",
    "    X_batch = batch\n",
    "    # y_batch = y_batch.numpy()\n",
    "    y_pred = clf.predict(X_batch.to_dense().numpy())\n",
    "    preds.append(y_pred)\n",
    "preds = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5ba519a1-3e26-40e2-82f4-cbc546dcd3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102f98e5e2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>033b399113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c125e29be2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b91e2b0679</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1a46141274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>0bfb1006b9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>f1df499466</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>de4da367a4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>07a8ec4593</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>cd06512bbb</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID  sentiment\n",
       "0     102f98e5e2          1\n",
       "1     033b399113          0\n",
       "2     c125e29be2          1\n",
       "3     b91e2b0679         -1\n",
       "4     1a46141274          0\n",
       "...          ...        ...\n",
       "2743  0bfb1006b9         -1\n",
       "2744  f1df499466         -1\n",
       "2745  de4da367a4         -1\n",
       "2746  07a8ec4593          1\n",
       "2747  cd06512bbb         -1\n",
       "\n",
       "[2748 rows x 2 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_conversion = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: -1\n",
    "}\n",
    "submission_df = pd.DataFrame({\"textID\": test_ds.df[\"textID\"], \"sentiment\": preds})\n",
    "submission_df[\"sentiment\"] = submission_df['sentiment'].map(target_conversion)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "967fc552-16cc-4727-ab04-b5632b4ca523",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "25e699a6-5bb7-4651-93ea-b730aa29df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "100%|██████████████████████████████████████| 35.5k/35.5k [00:00<00:00, 51.2kB/s]\n",
      "Successfully submitted to EURECOM AML 2022:: Challenge 3"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c eurecom-aml-2022-challenge-3 -f submission.csv -m \"Reimplement baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867c4cf-a0a8-4f09-9313-9cbe80345b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
